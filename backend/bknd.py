# -*- coding: utf-8 -*-
"""bknd.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R1gJ8zW1duH-Rf5mAO_c9uMunOWyK6FM
"""

# Importación de librerías necesarias
import re

import gradio as gr
import wikipedia
from datasets import Dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)


# Recoleccion de datos desde Wikipedia
def recolectar_articulos(tema, num_articulos=5):
    wikipedia.set_lang("es")
    articulos = []
    titulos = wikipedia.search(tema, results=num_articulos)
    for titulo in titulos:
        try:
            contenido = wikipedia.page(titulo).content
            articulos.append(contenido)
        except:
            continue
    return "\n".join(articulos)


# preprocesado del texto
def limpiar_texto(texto):
    texto = re.sub(r"\s+", " ", texto)  # eliminacion de espacios extra
    texto = re.sub(r"\[[^\]]*\]", "", texto)  # elimina las referencias [1], [2], ...
    return texto


# recolección y limpieza del corpus
tema = "reciclaje"
texto_crudo = recolectar_articulos(tema)
texto_limpio = limpiar_texto(texto_crudo)

# crear dataset para entrenamiento
datos = {"text": [texto_limpio]}
dataset = Dataset.from_dict(datos)

# Cargar el modelo preentrenado pequeño
modelo_id = "distilgpt2"
tokenizer = AutoTokenizer.from_pretrained(modelo_id)
modelo = AutoModelForCausalLM.from_pretrained(modelo_id)


# tokenizar el dataset
def tokenizar_funcion(ejemplo):
    return tokenizer(ejemplo["text"], truncation=True)


dataset_tokenizado = dataset.map(tokenizar_funcion, batched=True)

# fine-tuning del modelo
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
argumentos_entrenamiento = TrainingArguments(
    output_dir="./modelo_entrenado",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=2,
    save_steps=500,
    save_total_limit=1,
    logging_steps=100,
    prediction_loss_only=True,
)

tokenizer.pad_token = tokenizer.eos_token

entrenador = Trainer(
    model=modelo,
    args=argumentos_entrenamiento,
    train_dataset=dataset_tokenizado,
    data_collator=data_collator,
)

# entrenar el modelo
entrenador.train()


def responder(prompt):
    entradas = tokenizer(prompt, return_tensors="pt")
    salida = modelo.generate(**entradas, max_new_tokens=50)
    respuesta = tokenizer.decode(salida[0], skip_special_tokens=True)
    return respuesta


# interfaz gradio
interfaz = gr.Interface(
    fn=responder,
    inputs="text",
    outputs="text",
    title="Chat con Modelo Compacto",
    description="Escribe algo para conversar con el modelo entrenado.",
)
interfaz.launch()
